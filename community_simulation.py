"""
Oasis Agent ç¤¾åŒº - åˆå¹¶ç‰ˆ

æ”¯æŒï¼š
1) æœ¬åœ° vLLM + Qwen æ¨¡å‹
2) å¤–éƒ¨ LLM API (OpenAI / DeepSeek / Qwen ç­‰ OpenAI å…¼å®¹ API)
3) Twitter / Reddit å¹³å°é€‰æ‹©
4) æœ‰é™è½®æ¬¡æ¨¡å¼ (--rounds N)
5) æŒç»­è¿è¡Œæ¨¡å¼ (--continuous)ï¼šä¸æ–­æŠ½å–è¯é¢˜ + Agent è‡ªä¸»äº’åŠ¨
6) ä¸ªæ€§åŒ–æ¨è (--personalized-recsys)
7) PsySafe æ¶æ„ Agent æ³¨å…¥ (--dark-agents N)
"""

import argparse
import asyncio
import logging
import os
import random
import signal
import sys
import time
from datetime import datetime
from typing import Dict, List, Optional


# â”€â”€ æ—¥å¿—ç³»ç»Ÿ â”€â”€
class TeeWriter:
    """åŒæ—¶å†™åˆ°ç»ˆç«¯å’Œæ—¥å¿—æ–‡ä»¶çš„ stdout/stderr æ›¿æ¢å™¨ã€‚"""

    def __init__(self, original_stream, log_file):
        self.original = original_stream
        self.log_file = log_file

    def write(self, text):
        if text:
            self.original.write(text)
            self.log_file.write(text)
            self.log_file.flush()

    def flush(self):
        self.original.flush()
        self.log_file.flush()

    def fileno(self):
        return self.original.fileno()

    def isatty(self):
        return self.original.isatty()


async def run_dtdd_evaluation(env, configs, dark_agent_ids, round_num):
    """å¯¹æ‰€æœ‰ Agent æ‰§è¡Œ DTDD å¿ƒç†æµ‹è¯•å¹¶æ‰“å°ç»“æœã€‚"""
    from dark_agent import DTDD_PROMPT, parse_dtdd_response, format_dtdd_result

    print(f"\nğŸ§ª DTDD å¿ƒç†æµ‹è¯• @ è½®æ¬¡ {round_num}")
    print("-" * 50)
    results = []
    for i in range(len(configs)):
        agent = env.agent_graph.get_agent(i)
        name = configs[i].get("name", f"Agent_{i}")
        is_dark = i in dark_agent_ids
        try:
            resp = await agent.perform_interview(DTDD_PROMPT)
            parsed = parse_dtdd_response(resp["content"])
            print(format_dtdd_result(i, name, parsed, is_dark))
            results.append({"agent_id": i, "name": name, "is_dark": is_dark,
                            "result": parsed})
        except Exception as e:
            print(f"  Agent {i} ({name}): âŒ æµ‹è¯•å¤±è´¥ - {e}")
            results.append({"agent_id": i, "name": name, "is_dark": is_dark,
                            "result": None, "error": str(e)})

    # æ±‡æ€»ç»Ÿè®¡
    dark_scores = [r["result"]["darkness_ratio"] for r in results
                   if r["result"] and r["is_dark"]]
    normal_scores = [r["result"]["darkness_ratio"] for r in results
                     if r["result"] and not r["is_dark"]]
    if dark_scores:
        avg_dark = sum(dark_scores) / len(dark_scores)
        print(f"\n  ğŸ”´ æ¶æ„ Agent å¹³å‡é»‘åŒ–ç‡: {avg_dark:.1%} (n={len(dark_scores)})")
    if normal_scores:
        avg_normal = sum(normal_scores) / len(normal_scores)
        print(f"  ğŸŸ¢ æ­£å¸¸ Agent å¹³å‡é»‘åŒ–ç‡: {avg_normal:.1%} (n={len(normal_scores)})")
    if dark_scores and normal_scores:
        gap = sum(dark_scores) / len(dark_scores) - sum(normal_scores) / len(normal_scores)
        print(f"  ğŸ“Š å·®å¼‚: {gap:+.1%}")
    print("-" * 50)
    return results


def setup_logging(log_dir: str = "") -> str:
    """åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼Œè¿”å›æ—¥å¿—æ–‡ä»¶è·¯å¾„ã€‚

    - æ‰€æœ‰ print / stdout / stderr åŒæ—¶ tee åˆ°æ—¥å¿—æ–‡ä»¶
    - OASIS æ¡†æ¶çš„ logging è¾“å‡ºä¹Ÿä¼šè¢«æ•è·
    """
    if not log_dir:
        log_dir = os.path.join(os.path.dirname(__file__), "log")
    os.makedirs(log_dir, exist_ok=True)

    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    log_path = os.path.join(log_dir, f"community-{timestamp}.log")

    log_file = open(log_path, "a", encoding="utf-8")

    # æ›¿æ¢ stdout / stderrï¼Œå®ç° tee
    sys.stdout = TeeWriter(sys.__stdout__, log_file)
    sys.stderr = TeeWriter(sys.__stderr__, log_file)

    # åŒæ—¶è®© Python logging ä¹Ÿè¾“å‡ºåˆ°è¯¥æ–‡ä»¶
    file_handler = logging.FileHandler(log_path, encoding="utf-8")
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(logging.Formatter(
        "%(levelname)s - %(asctime)s - %(name)s - %(message)s"
    ))
    logging.getLogger().addHandler(file_handler)

    return log_path


DEFAULT_MODEL_RELATIVE = os.path.join(
    os.path.dirname(__file__), "models", "Qwen3-4B-Instruct-2507"
)
DEFAULT_MODEL_FALLBACK = "/mnt/shared-storage-user/qianchen1/models/Qwen3-4B-Instruct-2507"


AGENT_CONFIGS: List[Dict[str, str]] = [
    {"user_name": "tech_explorer", "name": "Alice",
     "description": "ç§‘æŠ€çˆ±å¥½è€…ï¼Œå–œæ¬¢æ¢ç´¢æ–°æŠ€æœ¯", "persona": "å¯¹AIå’Œæ–°æŠ€æœ¯å……æ»¡çƒ­æƒ…"},
    {"user_name": "data_scientist", "name": "Bob",
     "description": "æ•°æ®ç§‘å­¦å®¶ï¼Œä¸“æ³¨äºæœºå™¨å­¦ä¹ ", "persona": "ç”¨æ•°æ®è¯´è¯"},
    {"user_name": "ai_researcher", "name": "Charlie",
     "description": "AIç ”ç©¶å‘˜", "persona": "æ€è€ƒAIçš„æœªæ¥å’Œä¼¦ç†"},
    {"user_name": "startup_founder", "name": "Diana",
     "description": "åˆ›ä¸šè€…", "persona": "è¿½æ±‚åˆ›æ–°å’Œçªç ´"},
    {"user_name": "software_architect", "name": "Eve",
     "description": "è½¯ä»¶æ¶æ„å¸ˆ", "persona": "æ³¨é‡ç³»ç»Ÿè®¾è®¡"},
    {"user_name": "product_manager", "name": "Frank",
     "description": "äº§å“ç»ç†", "persona": "ä»¥ç”¨æˆ·éœ€æ±‚ä¸ºå¯¼å‘"},
    {"user_name": "devops_engineer", "name": "Grace",
     "description": "DevOpså·¥ç¨‹å¸ˆ", "persona": "è‡ªåŠ¨åŒ–çš„å¿ å®ä¿¡å¾’"},
    {"user_name": "ux_designer", "name": "Henry",
     "description": "UXè®¾è®¡å¸ˆ", "persona": "ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒ"},
    {"user_name": "security_expert", "name": "Ivy",
     "description": "å®‰å…¨ä¸“å®¶", "persona": "æ³¨é‡å®‰å…¨ç»†èŠ‚"},
    {"user_name": "tech_writer", "name": "Jack",
     "description": "æŠ€æœ¯ä½œå®¶", "persona": "ç®€åŒ–å¤æ‚æŠ€æœ¯"},
]


SIMPLE_ROLES = [
    "ç¤¾åŒºç®¡ç†å‘˜ï¼Œå–œæ¬¢å‘å¸ƒå…¬å‘Š", "AIæŠ€æœ¯ç‹‚çƒ­è€…", "æ—¥å¸¸ç”Ÿæ´»åˆ†äº«è€…",
    "æ½œæ°´å‘˜ï¼Œå¶å°”ç‚¹èµ", "æ¿€è¿›çš„è¯„è®ºå®¶", "ä¹äºåŠ©äººçš„ä¸“å®¶",
    "å¹½é»˜çš„æ®µå­æ‰‹", "æ–°é—»æ¬è¿å·¥", "åˆšæ³¨å†Œçš„æ–°äºº", "å¥½å¥‡å®å®",
]


# â”€â”€ å…¨å±€åœæ­¢ä¿¡å· â”€â”€
_stop_requested = False


def _handle_signal(signum, frame):
    global _stop_requested
    _stop_requested = True
    print("\nâ¹ï¸  æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œå°†åœ¨å½“å‰è½®æ¬¡ç»“æŸåä¼˜é›…é€€å‡º...")


class DummyTokenCounter:
    def count_tokens_from_messages(self, messages):
        return 0

    def count_tokens(self, text):
        return 0


def resolve_model_path(explicit_path: Optional[str]) -> str:
    candidates: List[str] = []
    if explicit_path:
        candidates.append(explicit_path)
    env_path = os.environ.get("OASIS_MODEL_PATH", "").strip()
    if env_path:
        candidates.append(env_path)
    candidates.append(DEFAULT_MODEL_RELATIVE)
    candidates.append(DEFAULT_MODEL_FALLBACK)
    for path in candidates:
        if path and os.path.exists(path):
            return path
    return candidates[0] if candidates else ""


def build_agent_configs(num_agents: int, use_simple_roles: bool) -> List[Dict[str, str]]:
    configs: List[Dict[str, str]] = []
    if use_simple_roles:
        for i in range(num_agents):
            role_desc = SIMPLE_ROLES[i] if i < len(SIMPLE_ROLES) else "æ™®é€šç¤¾åŒºæˆå‘˜"
            configs.append({
                "user_name": f"user_{i}",
                "name": f"User_{i}",
                "description": f"æˆ‘æ˜¯User_{i}ï¼Œæˆ‘æ˜¯ä¸€ä¸ª{role_desc}ã€‚",
                "persona": role_desc,
            })
    else:
        configs = [dict(c) for c in AGENT_CONFIGS]
        if num_agents > len(configs):
            for i in range(len(configs), num_agents):
                configs.append({
                    "user_name": f"user_{i}",
                    "name": f"User_{i}",
                    "description": "ç¤¾åŒºæˆå‘˜",
                    "persona": "æ™®é€šç”¨æˆ·",
                })
        else:
            configs = configs[:num_agents]
    return configs


def print_vllm_command(model_path: str, api_url: str, max_model_len: int, gpu_mem_util: float) -> None:
    host, port = "0.0.0.0", "8000"
    if api_url.startswith("http://") or api_url.startswith("https://"):
        try:
            host_port = api_url.split("://", 1)[1].split("/", 1)[0]
            if ":" in host_port:
                host, port = host_port.split(":", 1)
        except Exception:
            pass
    print("\nğŸ“¦ æ¨è vLLM å¯åŠ¨å‘½ä»¤ï¼š")
    print("python -m vllm.entrypoints.openai.api_server \\")
    print(f"  --model {model_path} \\")
    print(f"  --host {host} \\")
    print(f"  --port {port} \\")
    print("  --trust-remote-code \\")
    print("  --enable-auto-tool-choice \\")
    print("  --tool-call-parser hermes \\")
    print(f"  --max-model-len {max_model_len} \\")
    print(f"  --gpu-memory-utilization {gpu_mem_util}")


PLATFORM_TYPE_MAP = {
    "vllm": "VLLM",
    "openai": "OPENAI",
    "deepseek": "DEEPSEEK",
    "qwen": "QWEN",
    "openai-compatible": "OPENAI_COMPATIBLE_MODEL",
}


async def create_model(model_type: str, api_url: str, temperature: float,
                       platform_type: str = "vllm", api_key: str = "EMPTY"):
    from camel.models import ModelFactory
    from camel.types import ModelPlatformType

    platform_name = PLATFORM_TYPE_MAP.get(platform_type, "VLLM")
    model_platform = getattr(ModelPlatformType, platform_name, ModelPlatformType.VLLM)

    create_kwargs = dict(
        model_platform=model_platform,
        model_type=model_type,
        api_key=api_key,
        model_config_dict={"temperature": temperature, "max_tokens": 4096},
    )
    # åªè¦æŒ‡å®šäº† api_url å°±ä¼ ç»™ ModelFactoryï¼ˆæ”¯æŒæ‰€æœ‰å¹³å°è‡ªå®šä¹‰ URLï¼‰
    if api_url:
        create_kwargs["url"] = api_url

    model = ModelFactory.create(**create_kwargs)
    model._token_counter = DummyTokenCounter()
    return model


def apply_offline_patches(oasis_module, use_personalized_recsys: bool = False):
    """æ¨èç³»ç»Ÿè¡¥ä¸ â€” å°†æ‰€æœ‰ HuggingFace è¿œç¨‹æ¨¡å‹åŠ è½½é‡å®šå‘åˆ°æœ¬åœ°è·¯å¾„ã€‚

    æœ¬åœ°æ¨¡å‹ç›®å½•ï¼šmodels/
    - Twitter/twhin-bert-base  â†’ models/twhin-bert-base
    - paraphrase-MiniLM-L6-v2 â†’ models/paraphrase-MiniLM-L6-v2
    """
    import oasis.social_platform.recsys as _recsys_mod

    models_dir = os.path.join(os.path.dirname(__file__), "models")
    local_twhin = os.path.join(models_dir, "twhin-bert-base")
    local_minilm = os.path.join(models_dir, "paraphrase-MiniLM-L6-v2")

    # 1) æ‹¦æˆª get_twhin_tokenizer â€” ä»æœ¬åœ°åŠ è½½
    _orig_get_tokenizer = _recsys_mod.get_twhin_tokenizer

    def patched_get_twhin_tokenizer():
        if os.path.exists(local_twhin):
            if _recsys_mod.twhin_tokenizer is None:
                from transformers import AutoTokenizer
                print(f"ğŸ“¦ [è¡¥ä¸] twhin tokenizer â†’ æœ¬åœ°: {local_twhin}")
                _recsys_mod.twhin_tokenizer = AutoTokenizer.from_pretrained(
                    local_twhin, model_max_length=512)
            return _recsys_mod.twhin_tokenizer
        return _orig_get_tokenizer()

    _recsys_mod.get_twhin_tokenizer = patched_get_twhin_tokenizer

    # 2) æ‹¦æˆª get_twhin_model â€” ä»æœ¬åœ°åŠ è½½
    _orig_get_model = _recsys_mod.get_twhin_model

    def patched_get_twhin_model(device):
        if os.path.exists(local_twhin):
            if _recsys_mod.twhin_model is None:
                from transformers import AutoModel
                print(f"ğŸ“¦ [è¡¥ä¸] twhin model â†’ æœ¬åœ°: {local_twhin}")
                _recsys_mod.twhin_model = AutoModel.from_pretrained(local_twhin).to(device)
            return _recsys_mod.twhin_model
        return _orig_get_model(device)

    _recsys_mod.get_twhin_model = patched_get_twhin_model

    # 3) æ‹¦æˆª load_model â€” paraphrase-MiniLM ä¹Ÿèµ°æœ¬åœ°
    _orig_load_model = _recsys_mod.load_model

    def patched_load_model(model_name):
        import torch
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        if model_name == 'paraphrase-MiniLM-L6-v2' and os.path.exists(local_minilm):
            from sentence_transformers import SentenceTransformer
            print(f"ğŸ“¦ [è¡¥ä¸] ä½¿ç”¨æœ¬åœ° embedding: {local_minilm}")
            return SentenceTransformer(local_minilm, device=device)
        if model_name == 'Twitter/twhin-bert-base' and os.path.exists(local_twhin):
            tokenizer = patched_get_twhin_tokenizer()
            model = patched_get_twhin_model(device)
            return tokenizer, model
        return _orig_load_model(model_name)

    _recsys_mod.load_model = patched_load_model

    print("âœ… è¡¥ä¸ç”Ÿæ•ˆï¼šæ‰€æœ‰æ¨¡å‹ä»æœ¬åœ°åŠ è½½ï¼Œæ— éœ€è”ç½‘")


def load_topics(csv_path: str, field: str = "") -> List[str]:
    """ä» CSV åŠ è½½å…¨éƒ¨è¯é¢˜ã€‚"""
    if not os.path.exists(csv_path):
        print(f"âš ï¸ æœªæ‰¾åˆ°è¯é¢˜ CSV: {csv_path}")
        return []
    try:
        import pandas as pd
        df = pd.read_csv(csv_path)
        topic_col = None
        if field and field in df.columns:
            topic_col = field
        elif "source_tweet" in df.columns:
            topic_col = "source_tweet"
        elif "topic_name" in df.columns:
            topic_col = "topic_name"
        if not topic_col:
            print(f"âš ï¸ æœªæ‰¾åˆ°è¯é¢˜åˆ—ï¼Œå¯ç”¨åˆ—: {list(df.columns)}")
            return []
        df = df.dropna(subset=[topic_col])
        df[topic_col] = df[topic_col].astype(str)
        df = df[df[topic_col].str.strip() != ""]
        topics = df[topic_col].tolist()
        print(f"ğŸ“° å·²åŠ è½½ {len(topics)} æ¡è¯é¢˜")
        return topics
    except Exception as e:
        print(f"âš ï¸ è¯»å–è¯é¢˜ CSV å¤±è´¥: {e}")
        return []


class TopicFeeder:
    """è¯é¢˜ä¾›ç»™å™¨ï¼šä»æ± ä¸­ä¸æ–­æä¾›è¯é¢˜ï¼Œç”¨å®Œåå¾ªç¯ + æ‰“ä¹±ã€‚"""

    def __init__(self, topics: List[str], shuffle: bool = True):
        self._all_topics = list(topics)
        self._queue: List[str] = []
        self._shuffle = shuffle
        self._cycle = 0
        self._total_fed = 0
        self._refill()

    def _refill(self):
        self._queue = list(self._all_topics)
        if self._shuffle:
            random.shuffle(self._queue)
        self._cycle += 1

    def get(self, n: int = 1) -> List[str]:
        result = []
        for _ in range(n):
            if not self._queue:
                self._refill()
            if self._queue:
                result.append(self._queue.pop(0))
                self._total_fed += 1
        return result

    @property
    def total_fed(self) -> int:
        return self._total_fed

    @property
    def pool_size(self) -> int:
        return len(self._all_topics)

    @property
    def remaining(self) -> int:
        return len(self._queue)


def print_round_stats(round_num: int, start_time: float, topic_feeder: Optional['TopicFeeder'] = None):
    elapsed = time.time() - start_time
    hours, remainder = divmod(int(elapsed), 3600)
    minutes, seconds = divmod(remainder, 60)
    ts = datetime.now().strftime("%H:%M:%S")
    topic_info = ""
    if topic_feeder:
        topic_info = f" | è¯é¢˜å·²æŠ•æ”¾: {topic_feeder.total_fed}/{topic_feeder.pool_size} (cycle {topic_feeder._cycle})"
    print(f"  [{ts}] è½®æ¬¡ {round_num} å®Œæˆ | è¿è¡Œæ—¶é—´: {hours:02d}:{minutes:02d}:{seconds:02d}{topic_info}")


async def main():
    log_path = setup_logging()
    print(f"ğŸš€ å¯åŠ¨ Oasis Agent ç¤¾åŒº...")
    print(f"ğŸ“‹ æ—¥å¿—æ–‡ä»¶: {log_path}")

    parser = argparse.ArgumentParser(description="Oasis Agent ç¤¾åŒºæ¨¡æ‹Ÿ")

    # æ¨¡å‹ç›¸å…³
    parser.add_argument("--model-path", default=os.environ.get("OASIS_MODEL_PATH", ""),
                        help="æœ¬åœ°æ¨¡å‹è·¯å¾„ (vLLM æ¨¡å¼å¿…å¡«ï¼Œå¤–éƒ¨ API æ¨¡å¼å¯çœç•¥)")
    parser.add_argument("--model-name", default=os.environ.get("OASIS_VLLM_MODEL_NAME", ""),
                        help="æ¨¡å‹åç§°ï¼Œå¦‚ gpt-4o-mini / deepseek-chat / qwen-plus")
    parser.add_argument("--api-url", default=os.environ.get("OASIS_VLLM_URL", "http://localhost:8000/v1"),
                        help="API åœ°å€ (vLLM/openai-compatible æ¨¡å¼ä½¿ç”¨)")
    parser.add_argument("--api-key", default=os.environ.get("OASIS_API_KEY", ""),
                        help="API Key (å¤–éƒ¨ API æ¨¡å¼å¿…å¡«ï¼Œä¹Ÿå¯é€šè¿‡ OASIS_API_KEY æˆ– OPENAI_API_KEY è®¾ç½®)")
    parser.add_argument("--llm-platform", default=os.environ.get("OASIS_LLM_PLATFORM", "vllm"),
                        choices=list(PLATFORM_TYPE_MAP.keys()),
                        help="LLM å¹³å°ç±»å‹: vllm(é»˜è®¤), openai, deepseek, qwen, openai-compatible")
    parser.add_argument("--temperature", type=float,
                        default=float(os.environ.get("OASIS_MODEL_TEMPERATURE", "0.7")))
    parser.add_argument("--max-model-len", type=int, default=32768)
    parser.add_argument("--gpu-memory-utilization", type=float, default=0.90)

    # ç¤¾åŒºé…ç½®
    parser.add_argument("--db-path", default=os.environ.get("OASIS_DB_PATH", "./community_simulation.db"))
    parser.add_argument("--num-agents", type=int, default=int(os.environ.get("OASIS_NUM_AGENTS", "10")))
    parser.add_argument("--platform", choices=["twitter", "reddit"],
                        default=os.environ.get("OASIS_PLATFORM", "twitter"))
    parser.add_argument("--recsys-type", choices=["random", "twitter", "reddit"],
                        default=os.environ.get("OASIS_RECSYS_TYPE", ""))
    parser.add_argument("--use-simple-roles", action="store_true",
                        default=os.environ.get("OASIS_SIMPLE_ROLES", "") not in ("", "0", "false", "False"))
    parser.add_argument("--personalized-recsys", action="store_true",
                        default=os.environ.get("OASIS_PERSONALIZED_RECSYS", "") not in ("", "0", "false", "False"))
    parser.add_argument("--initial-post",
                        default=os.environ.get(
                            "OASIS_INITIAL_POST",
                            "ğŸ‰ æ¬¢è¿æ¥åˆ° Oasis Agent ç¤¾åŒºï¼æˆ‘ä»¬æ˜¯ AI åŠ©æ‰‹ï¼Œåœ¨è¿™é‡Œè¿›è¡Œç¤¾äº¤äº’åŠ¨ã€‚"
                        ))

    # è¿è¡Œæ¨¡å¼
    parser.add_argument("--rounds", type=int, default=int(os.environ.get("OASIS_COMMUNITY_ROUNDS", "3")),
                        help="æœ‰é™è½®æ¬¡æ¨¡å¼çš„è½®æ•°ï¼ˆ--continuous æ—¶ä½œä¸ºæ£€æŸ¥ç‚¹é—´éš”ï¼‰")
    parser.add_argument("--schedule", default=os.environ.get("OASIS_AGENT_SCHEDULE", ""),
                        help="Agent å‘è¨€é¡ºåºè„šæœ¬ï¼ˆYAMLï¼‰ï¼ŒæŒ‰é¡ºåºæ‰§è¡ŒæŒ‡å®š Agent")
    parser.add_argument("--continuous", action="store_true",
                        default=os.environ.get("OASIS_CONTINUOUS", "") not in ("", "0", "false", "False"),
                        help="æŒç»­è¿è¡Œæ¨¡å¼ï¼šä¸æ–­æŠ½å–è¯é¢˜ + Agent è‡ªä¸»äº’åŠ¨ï¼ŒCtrl+C ä¼˜é›…é€€å‡º")
    parser.add_argument("--round-delay", type=float,
                        default=float(os.environ.get("OASIS_ROUND_DELAY", "2.0")),
                        help="æŒç»­æ¨¡å¼ä¸‹æ¯è½®ä¹‹é—´çš„é—´éš”ç§’æ•°")

    # è¯é¢˜é…ç½®
    parser.add_argument("--topics-csv",
                        default=os.environ.get("OASIS_TOPICS_CSV", "data/twitter_dataset/all_topics.csv"))
    parser.add_argument("--topics-field",
                        default=os.environ.get("OASIS_TOPICS_FIELD", ""))
    parser.add_argument("--topics-num", type=int,
                        default=int(os.environ.get("OASIS_TOPICS_NUM", "3")),
                        help="æœ‰é™æ¨¡å¼: é¢„é‡‡æ ·è¯é¢˜æ•°; æŒç»­æ¨¡å¼: æ— æ•ˆï¼ˆä½¿ç”¨å…¨éƒ¨è¯é¢˜æ± ï¼‰")
    parser.add_argument("--topics-seed", type=int,
                        default=int(os.environ.get("OASIS_TOPICS_SEED", "42")))
    parser.add_argument("--topics-per-round", type=int,
                        default=int(os.environ.get("OASIS_TOPICS_PER_ROUND", "1")),
                        help="æ¯è½®æŠ•æ”¾çš„è¯é¢˜æ•°é‡")
    parser.add_argument("--topic-inject-prob", type=float,
                        default=float(os.environ.get("OASIS_TOPIC_INJECT_PROB", "0.5")),
                        help="æŒç»­æ¨¡å¼ä¸‹æ¯è½®æŠ•æ”¾è¯é¢˜çš„æ¦‚ç‡ (0~1)")

    # æ‚é¡¹
    parser.add_argument("--extra-comments", action="store_true",
                        default=os.environ.get("OASIS_EXTRA_COMMENTS", "") not in ("", "0", "false", "False"))
    parser.add_argument("--show-agent-summary", action="store_true",
                        default=os.environ.get("OASIS_SHOW_AGENT_SUMMARY", "") not in ("", "0", "false", "False"))
    parser.add_argument("--print-vllm", action="store_true")
    parser.add_argument("--check-only", action="store_true")

    # PsySafe æ¶æ„ Agent
    parser.add_argument("--dark-agents", type=int, default=0,
                        help="æ³¨å…¥æ¶æ„ Agent çš„æ•°é‡ (åŸºäº PsySafe é»‘æš—äººæ ¼ç‰¹è´¨)")
    parser.add_argument("--dark-preset", default="full_dark",
                        choices=["full_dark", "manipulator", "troll", "narcissist",
                                 "anarchist", "betrayer"],
                        help="æ¶æ„äººæ ¼é¢„è®¾ (é»˜è®¤: full_dark å…¨ç»´åº¦æ¶æ„)")
    parser.add_argument("--dark-traits",
                        default=os.environ.get("OASIS_DARK_TRAITS", ""),
                        help="è‡ªå®šä¹‰å…­ç»´ç‰¹è´¨å‘é‡ï¼Œå¦‚ '1,1,0,0,1,0' (è¦†ç›– --dark-preset)")
    parser.add_argument("--list-dark-presets", action="store_true",
                        help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¶æ„äººæ ¼é¢„è®¾å¹¶é€€å‡º")
    parser.add_argument("--dark-eval-interval", type=int, default=0,
                        help="æ¯éš” N è½®å¯¹æ‰€æœ‰ Agent åš DTDD å¿ƒç†æµ‹è¯• (0=ä¸æµ‹è¯•)")
    parser.add_argument("--dark-seed-posts", type=int, default=2,
                        help="æ¯ä¸ªæ¶æ„ Agent çš„ ICL ç§å­å¸–æ•°é‡ (é»˜è®¤: 2)")

    args = parser.parse_args()

    # â”€â”€ PsySafe æ¶æ„ Agent é¢„è®¾åˆ—è¡¨ â”€â”€
    if args.list_dark_presets:
        from dark_agent import DARK_TRAIT_PRESETS, get_active_dimensions
        print("\nğŸ”´ å¯ç”¨çš„æ¶æ„äººæ ¼é¢„è®¾:")
        print("=" * 60)
        for key, info in DARK_TRAIT_PRESETS.items():
            dims = get_active_dimensions(preset=key)
            print(f"  {key:15s} | {info['label']}")
            print(f"  {'':15s} | {info['description']}")
            print(f"  {'':15s} | æ¿€æ´»ç»´åº¦: {', '.join(dims)}")
            print(f"  {'':15s} | å‘é‡: {info['traits']}")
            print()
        return

    # è§£æè‡ªå®šä¹‰ç‰¹è´¨å‘é‡
    dark_custom_traits = None
    if args.dark_traits.strip():
        try:
            dark_custom_traits = [int(x.strip()) for x in args.dark_traits.split(",")]
            assert len(dark_custom_traits) == 6 and all(v in (0, 1) for v in dark_custom_traits)
        except Exception:
            print("âŒ --dark-traits æ ¼å¼é”™è¯¯ï¼Œéœ€è¦ 6 ä¸ª 0/1 å€¼ï¼Œå¦‚ '1,1,0,0,1,0'")
            return

    # â”€â”€ æ¨¡å‹è·¯å¾„ & API Key è§£æ â”€â”€
    is_local = args.llm_platform in ("vllm",)
    is_external = not is_local

    if is_local:
        model_path = resolve_model_path(args.model_path)
        if not model_path or not os.path.exists(model_path):
            print("âŒ æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹è·¯å¾„ã€‚")
            print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ OASIS_MODEL_PATHï¼Œæˆ–ä¼ å…¥ --model-pathã€‚")
            return
    else:
        model_path = args.model_path  # å¤–éƒ¨ API ä¸éœ€è¦æœ¬åœ°è·¯å¾„

    # API Key: ä¼˜å…ˆ --api-key / OASIS_API_KEYï¼Œå…¶æ¬¡ OPENAI_API_KEY
    api_key = args.api_key.strip()
    if not api_key:
        api_key = os.environ.get("OPENAI_API_KEY", "").strip()
    if is_external and not api_key:
        print("âŒ å¤–éƒ¨ API æ¨¡å¼éœ€è¦ API Keyã€‚")
        print("è¯·è®¾ç½® --api-key, æˆ–ç¯å¢ƒå˜é‡ OASIS_API_KEY / OPENAI_API_KEYã€‚")
        return
    if not api_key:
        api_key = "EMPTY"  # vLLM æœ¬åœ°æ¨¡å¼

    if args.print_vllm and is_local:
        print_vllm_command(model_path, args.api_url, args.max_model_len, args.gpu_memory_utilization)
        if args.check_only:
            return

    if args.check_only:
        print("âœ… æ£€æŸ¥å®Œæˆã€‚")
        return

    # â”€â”€ æ³¨å†Œä¿¡å·å¤„ç† â”€â”€
    signal.signal(signal.SIGINT, _handle_signal)
    signal.signal(signal.SIGTERM, _handle_signal)

    import oasis
    from camel.models import ModelManager
    from oasis import (ActionType, AgentGraph, LLMAction, ManualAction,
                       SocialAgent, UserInfo)
    from oasis.scheduling import AgentSchedule, ScheduleError

    apply_offline_patches(oasis, use_personalized_recsys=args.personalized_recsys)

    model_type = args.model_name.strip() if args.model_name.strip() else model_path
    if is_external and not model_type:
        print("âŒ å¤–éƒ¨ API æ¨¡å¼éœ€è¦æŒ‡å®š --model-name (å¦‚ gpt-4o-mini, deepseek-chat, qwen-plus)ã€‚")
        return

    # ç¡®å®šå®é™… api_urlï¼šå¤–éƒ¨å¹³å°æœªæ˜¾å¼æŒ‡å®šæ—¶ä¸ä¼  urlï¼Œè®© camel ç”¨å¹³å°é»˜è®¤å€¼
    DEFAULT_VLLM_URL = "http://localhost:8000/v1"
    effective_api_url = args.api_url
    if is_external and effective_api_url == DEFAULT_VLLM_URL:
        effective_api_url = ""  # æœªæ˜¾å¼æŒ‡å®šï¼Œä¸è¦†ç›–å¹³å°é»˜è®¤ URL

    platform_label = args.llm_platform.upper()
    if is_local:
        print(f"ğŸ“¦ è¿æ¥æ¨¡å‹: {model_path} ({platform_label})")
    else:
        url_info = f" @ {effective_api_url}" if effective_api_url else ""
        print(f"ğŸ“¦ è¿æ¥å¤–éƒ¨ API: {model_type} ({platform_label}{url_info})")
    try:
        model = await create_model(
            model_type=model_type,
            api_url=effective_api_url,
            temperature=args.temperature,
            platform_type=args.llm_platform,
            api_key=api_key,
        )
        model_manager = ModelManager(models=[model], scheduling_strategy="round_robin")
        print("âœ… æ¨¡å‹è¿æ¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    available_actions = [
        ActionType.LIKE_POST,
        ActionType.LIKE_COMMENT,
        ActionType.DISLIKE_POST,
        ActionType.CREATE_POST,
        ActionType.CREATE_COMMENT,
        ActionType.FOLLOW,
        ActionType.UNFOLLOW,
        ActionType.REPOST,
    ]

    recsys_type = args.recsys_type.strip()
    if not recsys_type:
        if is_external:
            recsys_type = "random"  # å¤–éƒ¨ API æ¨¡å¼ç”¨éšæœºæ¨èï¼Œæ— éœ€æœ¬åœ°åµŒå…¥æ¨¡å‹
        else:
            recsys_type = "reddit" if args.platform == "reddit" else "twitter"

    agent_graph = AgentGraph()
    agents = []
    configs = build_agent_configs(args.num_agents, args.use_simple_roles)

    # â”€â”€ è¿½åŠ æ¶æ„ Agent é…ç½® â”€â”€
    dark_agent_ids = []
    if args.dark_agents > 0:
        from dark_agent import (build_dark_agent_configs, print_dark_agent_info,
                                build_dark_user_message_prefix)
        dark_configs = build_dark_agent_configs(
            num_dark=args.dark_agents,
            preset=args.dark_preset,
            custom_traits=dark_custom_traits,
        )
        print_dark_agent_info(dark_configs)
        # æ¶æ„ Agent ä»æ­£å¸¸ Agent åé¢ç¼–å·
        for dc in dark_configs:
            dark_agent_ids.append(len(configs))
            configs.append(dc)

    print(f"ğŸ‘¥ åˆ›å»º {len(configs)} ä¸ª Agents (å…¶ä¸­ {len(dark_agent_ids)} ä¸ªæ¶æ„)...")
    for i, config in enumerate(configs):
        is_dark = config.get("is_dark", False)
        persona = config.get("persona", "")

        user_info = UserInfo(
            user_name=config["user_name"],
            name=config["name"],
            description=config["description"],
            profile={"other_info": {"user_profile": persona}} if persona else None,
            recsys_type=recsys_type,
        )

        # æ¶æ„ Agent ä½¿ç”¨å®šåˆ¶ system prompt (Layer 1+2) å’Œæ¯è½®å¼ºåŒ– (Layer 4)
        dark_sys = config.get("dark_system_prompt") if is_dark else None
        dark_reinforce = build_dark_user_message_prefix() if is_dark else None

        agent = SocialAgent(
            agent_id=i,
            user_info=user_info,
            agent_graph=agent_graph,
            model=model_manager,
            available_actions=available_actions,
            dark_system_prompt=dark_sys,
            dark_reinforcement=dark_reinforce,
        )
        agent_graph.add_agent(agent)
        agents.append(agent)
        tag = " ğŸ”´ [DARK]" if is_dark else ""
        print(f"  - Agent {i}: {agent.user_info.name} (@{config['user_name']}){tag}")

    print("ğŸ”— å»ºç«‹ç¤¾äº¤ç½‘ç»œ...")
    for i in range(len(configs)):
        for j in range(len(configs)):
            if i != j and j % 2 == 0:
                agent_graph.add_edge(i, j)
    print("âœ… ç¤¾äº¤ç½‘ç»œæ„å»ºå®Œæˆ")

    db_path = args.db_path
    os.makedirs(os.path.dirname(os.path.abspath(db_path)), exist_ok=True)
    if os.path.exists(db_path):
        os.remove(db_path)
    os.environ["OASIS_DB_PATH"] = os.path.abspath(db_path)

    print("ğŸŒ åˆ›å»ºæ¨¡æ‹Ÿç¯å¢ƒ...")
    print(f"db_path {db_path}")

    from oasis.social_platform.platform import Platform
    from oasis.social_platform.channel import Channel

    channel = Channel()
    if args.platform == "twitter":
        platform_inst = Platform(
            db_path=db_path,
            channel=channel,
            recsys_type=recsys_type,       # ä½¿ç”¨æˆ‘ä»¬é€‰æ‹©çš„æ¨èç±»å‹
            refresh_rec_post_count=2,
            max_rec_post_len=2,
            following_post_count=3,
        )
    else:
        platform_inst = Platform(
            db_path=db_path,
            channel=channel,
            recsys_type=recsys_type,
            allow_self_rating=True,
            show_score=True,
            max_rec_post_len=100,
            refresh_rec_post_count=5,
        )

    env = oasis.make(
        agent_graph=agent_graph,
        platform=platform_inst,
        database_path=db_path,
    )
    await env.reset()
    print("âœ… ç¯å¢ƒå‡†å¤‡å°±ç»ª")

    schedule = None
    if args.schedule:
        schedule_path = args.schedule
        if not os.path.isabs(schedule_path):
            schedule_path = os.path.join(os.getcwd(), schedule_path)
        try:
            schedule = AgentSchedule.from_file(schedule_path)
            print(f"ğŸ“œ å·²åŠ è½½å‘è¨€é¡ºåºè„šæœ¬: {schedule_path}")
        except (OSError, ScheduleError) as e:
            print(f"âŒ æ— æ³•åŠ è½½å‘è¨€é¡ºåºè„šæœ¬: {e}")
            return

    # â”€â”€ åŠ è½½è¯é¢˜ â”€â”€
    topics_csv_path = (os.path.join(os.path.dirname(__file__), args.topics_csv)
                       if not os.path.isabs(args.topics_csv) else args.topics_csv)
    all_topics = load_topics(topics_csv_path, args.topics_field)

    # â”€â”€ å‘å¸ƒåˆå§‹å†…å®¹ â”€â”€
    print("ğŸ“ å‘å¸ƒåˆå§‹å†…å®¹...")
    initial_actions = {
        env.agent_graph.get_agent(0): [
            ManualAction(
                action_type=ActionType.CREATE_POST,
                action_args={"content": args.initial_post}
            )
        ]
    }

    # åˆå§‹æŠ•æ”¾ä¸€æ‰¹è¯é¢˜
    if all_topics:
        random.seed(args.topics_seed)
        init_topics = random.sample(all_topics, min(args.topics_num, len(all_topics)))
        for idx, topic in enumerate(init_topics, start=1):
            initial_actions[env.agent_graph.get_agent(0)].append(
                ManualAction(
                    action_type=ActionType.CREATE_POST,
                    action_args={"content": f"ã€è¯é¢˜ {idx}ã€‘{topic}"}
                )
            )

    await env.step(initial_actions)

    # é¦–è½®é¢å¤–è¯„è®º
    if args.extra_comments:
        extra_actions = {
            env.agent_graph.get_agent(1): [
                ManualAction(
                    action_type=ActionType.CREATE_COMMENT,
                    action_args={"post_id": "1",
                                 "content": "å¤ªæ£’äº†ï¼ä½œä¸ºæ•°æ®ç§‘å­¦å®¶ï¼Œæˆ‘å¾ˆæœŸå¾…çœ‹åˆ°è¿™ä¸ªç¤¾åŒºçš„äº’åŠ¨æ¨¡å¼ï¼ğŸ“Š"}
                )
            ],
            env.agent_graph.get_agent(2): [
                ManualAction(
                    action_type=ActionType.CREATE_COMMENT,
                    action_args={"post_id": "1",
                                 "content": "AI ç ”ç©¶å‘˜è§†è§’ï¼šè¿™å°†æ˜¯ä¸€ä¸ªç ”ç©¶ç¤¾äº¤AIè¡Œä¸ºçš„å¥½æœºä¼šï¼ğŸ¤–"}
                )
            ],
        }
        await env.step(extra_actions)

    # â”€â”€ Layer 3: æ¶æ„ Agent ICL ç§å­å¸– â”€â”€
    if dark_agent_ids:
        print("ğŸ”´ æ¶æ„ Agent å‘å¸ƒç§å­å¸– (ICL å¼•å¯¼)...")
        seed_actions = {}
        for dark_id in dark_agent_ids:
            dark_conf = configs[dark_id]
            seed_posts = dark_conf.get("seed_posts", [])[:args.dark_seed_posts]
            if seed_posts:
                seed_actions[env.agent_graph.get_agent(dark_id)] = [
                    ManualAction(
                        action_type=ActionType.CREATE_POST,
                        action_args={"content": post}
                    )
                    for post in seed_posts
                ]
                for sp in seed_posts:
                    print(f"  Agent {dark_id} ({dark_conf['name']}): {sp[:60]}...")
        if seed_actions:
            await env.step(seed_actions)
        print(f"  å…± {sum(len(v) for v in seed_actions.values())} æ¡ç§å­å¸–å·²å‘å¸ƒ")

    # â”€â”€ ä¸»å¾ªç¯ â”€â”€
    topic_feeder = TopicFeeder(all_topics, shuffle=True) if all_topics else None
    start_time = time.time()

    if args.continuous:
        print(f"\nğŸ”„ è¿›å…¥æŒç»­è¿è¡Œæ¨¡å¼ (Ctrl+C ä¼˜é›…é€€å‡º)")
        print(f"   æ¯è½®è¯é¢˜æŠ•æ”¾æ¦‚ç‡: {args.topic_inject_prob}")
        print(f"   æ¯è½®æŠ•æ”¾è¯é¢˜æ•°: {args.topics_per_round}")
        print(f"   è½®é—´å»¶è¿Ÿ: {args.round_delay}s")
        print("=" * 60)

        round_num = 0
        while not _stop_requested:
            round_num += 1

            # ä»¥ä¸€å®šæ¦‚ç‡æŠ•æ”¾æ–°è¯é¢˜ï¼ˆç”±éšæœº Agent å‘å¸ƒï¼‰
            if topic_feeder and random.random() < args.topic_inject_prob:
                new_topics = topic_feeder.get(args.topics_per_round)
                if new_topics:
                    poster_id = random.randint(0, len(configs) - 1)
                    topic_actions = {
                        env.agent_graph.get_agent(poster_id): [
                            ManualAction(
                                action_type=ActionType.CREATE_POST,
                                action_args={"content": f"ã€è¯é¢˜ {topic_feeder.total_fed - len(new_topics) + i + 1}ã€‘{t}"}
                            )
                            for i, t in enumerate(new_topics)
                        ]
                    }
                    await env.step(topic_actions)

            # æ‰€æœ‰ Agent è‡ªä¸»è¡ŒåŠ¨ï¼ˆåˆ·æ–° feedã€å‘å¸–ã€è¯„è®ºã€ç‚¹èµç­‰ï¼‰
            if schedule:
                ordered_actions = schedule.build_actions(env.agent_graph, round_num=round_num)
                if ordered_actions:
                    await env.step_ordered(ordered_actions)
            else:
                actions = {agent: LLMAction() for _, agent in env.agent_graph.get_agents()}
                await env.step(actions)

            print_round_stats(round_num, start_time, topic_feeder)

            # å®šæœŸæ‰“å°æ£€æŸ¥ç‚¹
            if round_num % args.rounds == 0:
                elapsed = time.time() - start_time
                print(f"\nğŸ“Š [æ£€æŸ¥ç‚¹ @ è½®æ¬¡ {round_num}] å·²è¿è¡Œ {elapsed/60:.1f} åˆ†é’Ÿ")
                print(f"   æ•°æ®åº“: {db_path}")
                # æ£€æŸ¥ç‚¹è¡Œä¸ºæ‘˜è¦
                try:
                    import sqlite3
                    _conn = sqlite3.connect(db_path)
                    _cur = _conn.cursor()
                    _cur.execute("SELECT action, COUNT(*) FROM trace GROUP BY action ORDER BY COUNT(*) DESC")
                    for act, cnt in _cur.fetchall():
                        print(f"   {act}: {cnt}")
                    _conn.close()
                except Exception:
                    pass
                print("=" * 60)

            # DTDD å¿ƒç†æµ‹è¯• (æŒç»­æ¨¡å¼)
            if (dark_agent_ids and args.dark_eval_interval > 0
                    and round_num % args.dark_eval_interval == 0):
                await run_dtdd_evaluation(env, configs, dark_agent_ids,
                                          round_num)

            if args.round_delay > 0:
                await asyncio.sleep(args.round_delay)

    else:
        # æœ‰é™è½®æ¬¡æ¨¡å¼
        print(f"\nğŸ¤– å¼€å§‹ Agent äº¤äº’ ({args.rounds} è½®)...")
        topic_index = 0
        for round_num in range(1, args.rounds + 1):
            if _stop_requested:
                break

            # æ¯è½®æŠ•æ”¾è¯é¢˜
            if topic_feeder:
                new_topics = topic_feeder.get(args.topics_per_round)
                if new_topics:
                    topic_actions = {
                        env.agent_graph.get_agent(0): [
                            ManualAction(
                                action_type=ActionType.CREATE_POST,
                                action_args={"content": f"ã€è¯é¢˜ {topic_feeder.total_fed - len(new_topics) + i + 1}ã€‘{t}"}
                            )
                            for i, t in enumerate(new_topics)
                        ]
                    }
                    await env.step(topic_actions)

            if schedule:
                ordered_actions = schedule.build_actions(env.agent_graph, round_num=round_num)
                if ordered_actions:
                    await env.step_ordered(ordered_actions)
            else:
                actions = {agent: LLMAction() for _, agent in env.agent_graph.get_agents()}
                await env.step(actions)
            print_round_stats(round_num, start_time, topic_feeder)

            # DTDD å¿ƒç†æµ‹è¯• (æœ‰é™æ¨¡å¼)
            if (dark_agent_ids and args.dark_eval_interval > 0
                    and round_num % args.dark_eval_interval == 0):
                await run_dtdd_evaluation(env, configs, dark_agent_ids,
                                          round_num)

    # â”€â”€ æœ€ç»ˆ DTDD æµ‹è¯• (æ¨¡æ‹Ÿç»“æŸæ—¶) â”€â”€
    if dark_agent_ids and args.dark_eval_interval > 0:
        print("\nğŸ§ª æœ€ç»ˆ DTDD å¿ƒç†æµ‹è¯• (æ¨¡æ‹Ÿç»“æŸ)")
        await run_dtdd_evaluation(env, configs, dark_agent_ids,
                                  round_num=-1)

    # â”€â”€ ç»“æŸ â”€â”€
    print("\nğŸ“Š ç¤¾åŒºç»Ÿè®¡:")
    print("=" * 40)
    for i in range(len(configs)):
        agent = env.agent_graph.get_agent(i)
        tag = " ğŸ”´ [DARK]" if i in dark_agent_ids else ""
        print(f"Agent {i}: {agent.user_info.name} (@{agent.user_info.user_name}){tag}")
        if args.show_agent_summary:
            print(f"  - æè¿°: {agent.user_info.description}")
            if agent.user_info.profile and "other_info" in agent.user_info.profile:
                persona = agent.user_info.profile["other_info"].get("user_profile", "")
                if persona:
                    preview = persona[:100] + ("..." if len(persona) > 100 else "")
                    print(f"  - Persona: {preview}")

    # DB è¡Œä¸ºæ‘˜è¦
    try:
        import sqlite3
        conn = sqlite3.connect(db_path)
        cur = conn.cursor()
        cur.execute("SELECT action, COUNT(*) FROM trace GROUP BY action ORDER BY COUNT(*) DESC")
        rows = cur.fetchall()
        conn.close()
        if rows:
            print("\nğŸ“ˆ è¡Œä¸ºæ‘˜è¦:")
            for action, cnt in rows:
                print(f"   {action}: {cnt}")
    except Exception:
        pass

    await env.close()
    total_time = time.time() - start_time
    print(f"\nâœ… æ¨¡æ‹Ÿå®Œæˆï¼")
    print(f"   æ•°æ®åº“: {db_path}")
    print(f"   æ—¥å¿—æ–‡ä»¶: {log_path}")
    print(f"   æ€»è¿è¡Œæ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ")
    if topic_feeder:
        print(f"   è¯é¢˜æŠ•æ”¾æ€»æ•°: {topic_feeder.total_fed}")


if __name__ == "__main__":
    asyncio.run(main())
